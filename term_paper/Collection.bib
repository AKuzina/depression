@article{Arsigny2007,
abstract = {In this work we present a new generalization of the geometric mean of positive numbers on symmetric positive-definite matrices, called Log-Euclidean. The approach is based on two novel algebraic structures on symmetric positive-definite matrices: first, a lie group structure which is compatible with the usual algebraic properties of this matrix space; second, a new scalar multiplication that smoothly extends the Lie group structure into a vector space structure. From bi-invariant metrics on the Lie group structure, we define the Log-Euclidean mean from a Riemannian point of view. This notion coincides with the usual Euclidean mean associated with the novel vector space structure. Furthermore, this means corresponds to an arithmetic mean in the domain of matrix logarithms. We detail the invariance properties of this novel geometric mean and compare it to the recently introduced affine-invariant mean. The two means have the same determinant and are equal in a number of cases, yet they are not identical in general. Indeed, the Log-Euclidean mean has a larger trace whenever they are not equal. Last but not least, the Log-Euclidean mean is much easier to compute.},
annote = {Тут рассматривают log-Eucledean metric, сравнивают ее с affine-invariant 

Надо понять, в чем разница, потому что я счита для обеих, вторая всегда лучше},
author = {Arsigny, Vincent and Fillard, Pierre and Pennec, Xavier and Ayache, Nicholas},
doi = {10.1137/050637996},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Arsigny et al. - 2007 - Geometric means in a novel vector space structure on symmetric positive-definite matrices.pdf:pdf},
isbn = {0895-4798},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {affine-invariant,bi-invariant,geometric mean,lie groups,log-euclidean,riemann,symmetric positive-definite matrices},
mendeley-tags = {affine-invariant,geometric mean,log-euclidean,riemann},
number = {1},
pages = {328--347},
title = {{Geometric means in a novel vector space structure on symmetric positive-definite matrices}},
url = {http://epubs.siam.org/doi/10.1137/050637996},
volume = {29},
year = {2007}
}
@article{Barachant2013,
abstract = {The use of spatial covariance matrix as a feature is investigated for motor imagery EEG-based classification in Brain-Computer Interface applications. A new kernel is derived by establishing a connection with the Riemannian geometry of symmetric positive definite matrices. Different kernels are tested, in combination with support vector machines, on a past BCI competition dataset. We demonstrate that this new ap- proach outperforms significantly state of the art results, effectively replacing the traditional spatial filtering approach.},
author = {Barachant, Alexandre and Bonnet, St{\'{e}}phane and Congedo, Marco and Jutten, Christian},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Barachant et al. - 2013 - Classification of covariance matrices using a Riemannian-based kernel for BCI applications.pdf:pdf},
journal = {Neurocomputing},
pages = {pp.172--178},
title = {{Classification of covariance matrices using a Riemannian-based kernel for BCI applications}},
year = {2013}
}
@article{Barachant2012,
abstract = {This paper presents a new classification framework for brain-computer interface (BCI) based on motor imagery. This framework involves the concept of Riemannian geometry in the manifold of covariance matrices. The main idea is to use spatial covariance matrices as EEG signal descriptors and to rely on Riemannian geometry to directly classify these matrices using the topology of the manifold of symmetric and positive definite (SPD) matrices. This framework allows to extract the spatial information contained in EEG signals without using spatial filtering. Two methods are proposed and compared with a reference method [multiclass Common Spatial Pattern (CSP) and Linear Discriminant Analysis (LDA)] on the multiclass dataset IIa from the BCI Competition IV. The first method, named minimum distance to Riemannian mean (MDRM), is an implementation of the minimum distance to mean (MDM) classification algorithm using Riemannian distance and Riemannian mean. This simple method shows comparable results with the reference method. The second method, named tangent space LDA (TSLDA), maps the covariance matrices onto the Riemannian tangent space where matrices can be vectorized and treated as Euclidean objects. Then, a variable selection procedure is applied in order to decrease dimensionality and a classification by LDA is performed. This latter method outperforms the reference method increasing the mean classification accuracy from 65.1{\%} to 70.2{\%}.},
author = {Barachant, Alexandre and Bonnet, St{\'{e}}phane and Congedo, Marco and Jutten, Christian},
doi = {10.1109/TBME.2011.2172210},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Barachant et al. - 2012 - Multiclass brain-computer interface classification by Riemannian geometry.pdf:pdf},
isbn = {0018-9294},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Brain computer interfaces,affine-invariant,classification,classification algorithms,covariance matrix,electroencephalography,information geometry,riemann},
mendeley-tags = {affine-invariant,classification,riemann},
number = {4},
pages = {920--928},
pmid = {22010143},
title = {{Multiclass brain-computer interface classification by Riemannian geometry}},
volume = {59},
year = {2012}
}
@article{Barachant2010,
abstract = {We develop a probabilistic framework for multiway analysis of high dimensional datasets. By exploiting a link between graphical models and tensor factorization models we can realize any arbitrary tensor factorization structure, and many popular models such as CP or TUCKER models with Euclidean error and their non-negative variants with KL error appear as special cases. Due to the duality between exponential families and Bregman divergences, we can cast the problem as inference in a model with Gaussian or Poisson components, where tensor factorisation reduces to a parameter estimation problem. We derive the generic form of update equations for multiplicative and alternating least squares. We also propose a straightforward matricisation procedure to convert element-wise equations into the matrix forms to ease implementation and parallelisation.},
author = {Barachant, Alexandre and Bonnet, St{\'{e}}phane and Congedo, Marco and Jutten, Christian},
doi = {10.1007/978-3-642-15995-4},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Barachant et al. - 2010 - Riemannian geometry applied to BCI classification.pdf:pdf},
isbn = {978-3-642-15994-7},
journal = {Lva/Ica 2010},
keywords = {cp,em algorithm,graphical models,nmf,non-negative decompositions,ntf,tensor factorisation,tucker},
pages = {346--353},
title = {{Riemannian geometry applied to BCI classification}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-15995-4},
year = {2010}
}
@article{Belkin2001,
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1.1.19.9400},
eprint = {arXiv:1011.1669v3},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Belkin, Niyogi - 2001 - Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Nips},
keywords = {classification},
mendeley-tags = {classification},
pages = {585--591},
pmid = {25246403},
title = {{Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.9400{\&}rep=rep1{\&}type=pdf},
volume = {14},
year = {2001}
}
@book{Berger1927,
author = {Berger, Marcel and Gostiaux, Bernard},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Berger, Gostiaux - 1927 - Differential Geometry Manifolds, Curves, and Surfaces.pdf.pdf:pdf},
title = {{Differential Geometry: Manifolds, Curves, and Surfaces.pdf}},
year = {1927}
}
@article{Bonnabel2008,
abstract = {This paper introduces a new metric and mean on the set of positive semidefinite matrices of fixed-rank. The proposed metric is derived from a well-chosen Riemannian quotient geometry that generalizes the reductive geometry of the positive cone and the associated natural metric. The resulting Riemannian space has strong geometrical properties: it is geodesically complete, and the metric is invariant with respect to all transformations that preserve angles (orthogonal transformations, scalings, and pseudoinversion). A meaningful approximation of the associated Riemannian distance is proposed, that can be efficiently numerically computed via a simple algorithm based on SVD. The induced mean preserves the rank, possesses the most desirable characteristics of a geometric mean, and is easy to compute.},
archivePrefix = {arXiv},
arxivId = {0807.4462},
author = {Bonnabel, Silvere and Sepulchre, Rodolphe},
doi = {10.1137/080731347},
eprint = {0807.4462},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Bonnabel, Sepulchre - 2008 - Riemannian Metric and Geometric Mean for Positive Semidefinite Matrices of Fixed Rank.pdf:pdf},
isbn = {9550121011},
issn = {0895-4798},
title = {{Riemannian Metric and Geometric Mean for Positive Semidefinite Matrices of Fixed Rank}},
url = {http://arxiv.org/abs/0807.4462},
year = {2008}
}
@article{Chavel,
author = {Chavel, Isaac},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Chavel - Unknown - Riemannian Geometry. A modern Introduction.pdf:pdf},
title = {{Riemannian Geometry. A modern Introduction}}
}
@article{Craddock2013,
author = {Craddock, Cameron and Benhajali, Yassine and Chu, Carlton and Chouinard, Francois and Alan, Evans and Jakab, Andr{\'{a}}s and Budhachandra, S. Khundrakpam and John, D. Lewis and Qingyang, Li and Milham, Michael and Yan, Chaogan and Bellec, Pierre},
journal = {Neuroinformatic},
title = {{The Neuro Bureau Preprocessing Initiative: open sharing of preprocessed neuroimaging data and derivatives}},
year = {2013}
}
@book{Faraut1995,
author = {Faraut, Jacques and Kor{\'{a}}nyi, Adam},
pages = {400},
title = {{Analysis on Symmetric Cones}},
year = {1995}
}
@article{Fletcher2004,
abstract = {Abstract. Diffusion tensor magnetic resonance imaging (DT-MRI) is$\backslash$nemerging as an important tool in medical image analysis of the brain.$\backslash$nHowever, relatively little work has been done on producing statistics$\backslash$nof diffusion tensors. A main difficulty is that the space of diffusion$\backslash$ntensors, i.e., the space of symmetric, positivedefinite matrices,$\backslash$ndoes not form a vector space. Therefore, standard linear statistical$\backslash$ntechniques do not apply. We show that the space of diffusion tensors$\backslash$nis a type of curved manifold known as a Riemannian symmetric space.$\backslash$nWe then develop methods for producing statistics, namely averages$\backslash$nand modes of variation, in this space. In our previous work we introduced$\backslash$nprincipal geodesic analysis, a generalization of principal component$\backslash$nanalysis, to compute the modes of variation of data in Lie groups.$\backslash$nIn this work we expand the method of principal geodesic analysis$\backslash$nto symmetric spaces and apply it to the computation of the variability$\backslash$nof diffusion tensor data. We expect that these methods will be useful$\backslash$nin the registration of diffusion tensor images, the production of$\backslash$nstatistical atlases from diffusion tensor data, and the quantification$\backslash$nof the anatomical variability caused by disease. 1},
author = {Fletcher, Thomas and Joshi, Sarang},
doi = {10.1007/978-3-540-27816-0_8},
file = {:Users/annakuzina/Google Drive/study/masters/Research/Literature/matrix classification/Forstner{\_}proncipal.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
pages = {87--98},
title = {{Principal Geodesic Analysis on Symmetric Spaces: Statistics of Diffusion Tensors}},
url = {http://link.springer.com/10.1007/978-3-540-27816-0{\_}8},
year = {2004}
}
@article{Forstner1999,
abstract = {The paper presents a metric for positive definite covariance matrices. It is a natural expression involving traces and joint eigenvalues of the matrices. It is shown to be the distance coming from a canonical invariant Riemannian metric on the space Sym+(n,R) of real symmetric positive definite matrices In contrast to known measures, collected e. g. in Grafarend 1972, the metric is invariant under affine transformations and inversion. It can be used for evaluating covariance matrices or for optimization of measurement designs.},
annote = {Пишут, что это они придумали эту метрику},
author = {F{\"{o}}rstner, Wolfgang and Moonen, Boudewijn},
doi = {10.1007/978-3-662-05296-9_31},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/F{\"{o}}rstner, Moonen - 1999 - A Metric for Covariance Matrices.pdf:pdf},
isbn = {978-3-642-07733-3},
journal = {Quo vadis geodesia},
keywords = {covariance matrices,exponential map-,lie groups,metric,ping,riemannian manifolds,symmetric spaces},
pages = {113--128},
title = {{A Metric for Covariance Matrices}},
url = {http://www.uni-stuttgart.de/gi/research/schriftenreihe/quo{\_}vadis/pdf/foerstner.pdf},
volume = {66},
year = {1999}
}
@article{Huang2016,
abstract = {Symmetric Positive Definite (SPD) matrix learning methods have become popular in many image and video processing tasks, thanks to their ability to learn appropriate statistical representations while respecting Riemannian geometry of underlying SPD manifolds. In this paper we build a Riemannian network architecture to open up a new direction of SPD matrix non-linear learning in a deep model. In particular, we devise bilinear mapping layers to transform input SPD matrices to more desirable SPD matrices, exploit eigenvalue rectification layers to apply a non-linear activation function to the new SPD matrices, and design an eigenvalue logarithm layer to perform Riemannian computing on the resulting SPD matrices for regular output layers. For training the proposed deep network, we exploit a new backpropagation with a variant of stochastic gradient descent on Stiefel manifolds to update the structured connection weights and the involved SPD matrix data. We show through experiments that the proposed SPD matrix network can be simply trained and outperform existing SPD matrix learning and state-of-the-art methods in three typical visual classification tasks.},
archivePrefix = {arXiv},
arxivId = {1608.04233},
author = {Huang, Zhiwu and {Van Gool}, Luc},
eprint = {1608.04233},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Van Gool - 2016 - A Riemannian Network for SPD Matrix Learning.pdf:pdf},
pages = {2036--2042},
title = {{A Riemannian Network for SPD Matrix Learning}},
url = {http://arxiv.org/abs/1608.04233},
year = {2016}
}
@article{Moakher2005,
abstract = {In this paper we introduce metric-based means for the space of positive-definite matrices. The mean associated with the Euclidean metric of the ambient space is the usual arithmetic mean. The mean associated with the Riemannian metric corresponds to the geometric mean. We discuss some invariance properties of the Riemannian mean and we use differential geometric tools to give a characterization of this mean.},
author = {Moakher, Maher},
doi = {10.1137/S0895479803436937},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Moakher - 2005 - A Differential Geometric Approach to the Geometric Mean of Symmetric Positive-Definite Matrices.pdf:pdf},
isbn = {0895-4798},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {geometric mean,positive-definite symmetric matrices,riemannian distance},
number = {3},
pages = {735--747},
title = {{A Differential Geometric Approach to the Geometric Mean of Symmetric Positive-Definite Matrices}},
volume = {26},
year = {2005}
}
@article{Moakher2005a,
author = {Moakher, Maher and Batchelor, Philipp},
doi = {10.1007/3-540-31272-2},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Moakher, Batchelor - 2005 - Symmetric Positive Definite Matrices From Geometry to Applications and Visualization.pdf:pdf},
isbn = {978-3-540-25032-6},
pages = {17},
title = {{Symmetric Positive Definite Matrices: From Geometry to Applications and Visualization}},
url = {http://discovery.ucl.ac.uk/84216/},
year = {2005}
}
@book{2005,
author = {Novikov, S.P and Taymanov, I.A.},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Novikov, Taymanov - 2005 - Modern geometric structures and fields.pdf:pdf},
title = {{Modern geometric structures and fields.}},
year = {2005}
}
@article{Pennec2004,
archivePrefix = {arXiv},
arxivId = {RR-5093, 2004},
author = {Pennec, Xavier},
eprint = {RR-5093, 2004},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Pennec - 2004 - Probabilities and Statistics on Riemannian A Geometric approach.pdf:pdf},
journal = {RESEARCH rapport},
number = {January},
title = {{Probabilities and Statistics on Riemannian A Geometric approach}},
year = {2004}
}
@article{Pennec2006,
annote = {Раздел 3.2},
author = {Pennec, Xavier and Fillard, Pierre and Ayache, Nicholas},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Pennec, Fillard, Ayache - 2006 - A Riemannian Framework For Tensor Computing.pdf:pdf},
journal = {Intl. Journal on Computer Vision},
number = {1},
pages = {41--66},
title = {{A Riemannian Framework For Tensor Computing}},
volume = {66},
year = {2006}
}
@article{Unknown,
author = {Pimkin, Artem and Belyaev, Mikhail and Dudonova, Julia and Gutman, Boris and Faskowitz, Joshua and Djahanshad, Neda and Tompson, Paul},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Pimkin et al. - 2017 - Classification of brain network structures using analysis of symmetric semidefinite matrices.pdf:pdf},
title = {{Classification of brain network structures using analysis of symmetric semidefinite matrices}},
year = {2017}
}
@book{Sternberg1964,
address = {NJ},
author = {Sternberg, Shlomo},
booktitle = {October},
file = {:Users/annakuzina/Google Drive/study/masters/Research/Literature/matrix classification/{\_}Shlomo{\_}Sternberg{\_}{\_}Lectures{\_}on{\_}Differential{\_}Geomet(BookSee.org).PDF:PDF},
publisher = {Prentice–Hall},
title = {{Lectures on Differential Geometry}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/9810234945},
year = {1964}
}
@article{Wang2012,
abstract = {We propose a novel discriminative learning approach to image set classification by modeling the image set with its natural second-order statistic, i.e. covariance matrix. Since nonsingular covariance matrices, a.k.a. symmetric positive definite (SPD) matrices, lie on a Riemannian manifold, classical learning algorithms cannot be directly utilized to classify points on the manifold. By exploring an efficient metric for the SPD matrices, i.e., Log-Euclidean Distance (LED), we derive a kernel function that explicitly maps the covariance matrix from the Riemannian manifold to a Euclidean space. With this explicit mapping, any learning method devoted to vector space can be exploited in either its linear or kernel formulation. Linear Discriminant Analysis (LDA) and Partial Least Squares (PLS) are considered in this paper for their feasibility for our specific problem. We further investigate the conventional linear subspace based set modeling technique and cast it in a unified framework with our covariance matrix based modeling. The proposed method is evaluated on two tasks: face recognition and object categorization. Extensive experimental results show not only the superiority of our method over state-of-the-art ones in both accuracy and efficiency, but also its stability to two real challenges: noisy set data and varying set size.},
annote = {Также использую 2 метрики 
Посмотреть, на кого ссылаются},
author = {Wang, Ruiping and Guo, Huimin and Davis, Larry S and Dai, Qionghai},
file = {:Users/annakuzina/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2012 - Covariance Discriminative Learning A Natural and Efficient Approach to Image Set Classification.pdf:pdf},
isbn = {9781467312288},
issn = {9781467312288},
journal = {Computer Vision and Pattern Recognition},
keywords = {affine-invariant,log-euclidean,riemann},
mendeley-tags = {affine-invariant,log-euclidean,riemann},
pages = {2496--2503},
title = {{Covariance Discriminative Learning : A Natural and Efficient Approach to Image Set Classification}},
year = {2012}
}
